---
title: "PSTAT131_HW5"
author: "Tao Wang"
date: '2022-05-11'
output:
  pdf_document: default
  html_document: default
---


### Exercise 1

Install and load the `janitor` package. Use its `clean_names()` function on the Pokémon data, and save the results to work with for the rest of the assignment. What happened to the data? Why do you think `clean_names()` is useful?

```{r, results='hide'}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
tidymodels_prefer()

```

```{r}
# 1. Install and load the `janitor` package.
# install.packages('janitor')
library(janitor)
set.seed(1234) # can be any number
```


```{r}
pokemon <- read.csv(file = "data/Pokemon.csv") 
head(pokemon)
#  Use its `clean_names()` function on the Pokémon data
pokemon<-clean_names(pokemon)
head(pokemon)
```
* What happened to the data? 
* Answer:
- By comparing the output with `clean_names()` and the output without `clean_names()`, we can see that the column names have changed by `clean_names()`function. 
- To be more specific, without using  `clean_names()`, the output of the column names are mixed uppercase, lowercase and special characters. This is very complicated. For example, we can see some names like `Sp..Def` and `Type.2`. However, by using `clean_names()`, all the columns become more formatted (consist of lowercase and underscore). For example, `Sp..Def`  changed to `sp_def` by using `clean_names()`.


* Why do you think `clean_names()` is useful?
* Answer:
- The `clean_names()` function makes the column names more formatted, which can help us make it easier to write code. We don't have to type both upper and lower case at same time, and we don't have to type too many special characters. It can improve our work efficiency.

### Exercise 2

Using the entire data set, create a bar chart of the outcome variable, `type_1`.

```{r}
# create a bar chart of the outcome variable, `type_1`.
pokemon %>% 
  ggplot(aes(x=type_1))+
  geom_bar()

```

* How many classes of the outcome are there? Are there any Pokémon types with very few Pokémon? If so, which ones?

* Answer:
- According to the graph, there are 18 classes of the outcome here.
- Flying Pokémon types with very few Pokémon.


For this assignment, we'll handle the rarer classes by simply filtering them out. Filter the entire data set to contain only Pokémon whose `type_1` is Bug, Fire, Grass, Normal, Water, or Psychic.

```{r}
pokemon <- pokemon %>% 
  filter(type_1 %in% c('Bug', 'Fire', 'Grass' , 'Normal', 'Water', 'Psychic'))
```


After filtering, convert `type_1` and `legendary` to factors.
```{r}
# convert `type_1` and `legendary` to factors
pokemon$type_1 <- factor(pokemon$type_1)
pokemon$legendary <- factor(pokemon$legendary)

# double check whether `type_1` and `legendary` are factors
is.factor(pokemon$type_1)
is.factor(pokemon$legendary)

```


### Exercise 3

Perform an initial split of the data. Stratify by the outcome variable. You can choose a proportion to use. Verify that your training and test sets have the desired number of observations.

Next, use *v*-fold cross-validation on the training set. Use 5 folds. Stratify the folds by `type_1` as well. *Hint: Look for a `strata` argument.* Why might stratifying the folds be useful?


```{r}
# Perform an initial split of the data. 
pokemon_split <- initial_split(pokemon,prop = 0.80, strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

# Verify that your training and test sets have the desired number of observations.
dim(pokemon)
dim(pokemon_train)
dim(pokemon_test)
```
*  Verify that your training and test sets have the desired number of observations.
- Each dataset has approximately the right number of observations; 
- For the training dataset, 364 is almost exactly 80% of the full data set, which contains 458 observations.
- For the testing dataset, 94 is almost exactly 20% of the full data set, which contains 458 observations.

```{r}
# Use *v*-fold cross-validation on the training set. Use 5 folds. 
# Stratify the folds by `type_1` as well.
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata = type_1)

```
* Why might stratifying the folds be useful?
- In previous question, we have already noticed that our responses classes are imbalanced. At this time, it will be useful for us to stratify the folds.

* Here are the reasons:
- According to the lecture and online resource, we know that "in stratified k-fold cross-validation, the partitions are selected so that the mean response value is approximately equal in all the partitions." (https://en.wikipedia.org/wiki/Cross-validation_(statistics))
It means that stratifying the folds can "ensure that each fold is an appropriate representative of the original data. (class distribution, mean, variance, etc) " (https://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation)


### Exercise 4

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`.

- Dummy-code `legendary` and `generation`;

- Center and scale all predictors.
```{r}
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + 
                           attack + speed + defense+ hp +sp_def, pokemon_train) %>% 
  step_dummy(legendary,generation) %>% 
  step_center(all_predictors())%>% 
  step_scale(all_predictors())
```




### Exercise 5

We'll be fitting and tuning an elastic net, tuning `penalty` and `mixture` (use `multinom_reg` with the `glmnet` engine).

Set up this model and workflow. Create a regular grid for `penalty` and `mixture` with 10 levels each; `mixture` should range from 0 to 1. For this assignment, we'll let `penalty` range from -5 to 5 (it's log-scaled).

How many total models will you be fitting when you fit these models to your folded data?

```{r}
# fitting and tuning an elastic net, tuning `penalty` and `mixture`
elastic_net_spec <- multinom_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

# Set up this model and workflow.
elastic_net_workflow <- workflow() %>% 
  add_recipe(pokemon_recipe) %>% 
  add_model(elastic_net_spec)

# Create a regular grid for `penalty` and `mixture` with 10 levels each; `mixture`
elastic_net_grid <- grid_regular(penalty(range = c(-5, 5)), mixture(range = c(0,1)), levels = 10)

```

* How many total models will you be fitting when you fit these models to your folded data?
* Answer: 500 models will be fit when we fit these models to your folded data.




### Exercise 6

Fit the models to your folded data using `tune_grid()`.

Use `autoplot()` on the results. What do you notice? Do larger or smaller values of `penalty` and `mixture` produce better accuracy and ROC AUC?

```{r}
# Fit the models to your folded data using `tune_grid()`.
tune_res <- tune_grid(
  elastic_net_workflow,
  resamples = pokemon_folds, 
  grid = elastic_net_grid
)

# Use `autoplot()` on the results. 
autoplot(tune_res)


```

* What do you notice? Do larger or smaller values of `penalty` and `mixture` produce better accuracy and ROC AUC?

- 1. We notice that as the amount of regularization increases, both accuracy and ROC_AUC will decrease.
- 2. According to this graph, for ROC_AUC, we can conclude that smaller values of `penalty` and `mixture` produce better ROC_AUC. However, for accuracy, we cannot make a conclusion just based on this graph. From the top of the graph, we can see that before the midpoint of 1*e-03 and 1e+00, larger values of `penalty` and `mixture` produce better accuracy, but after that point, smaller values of `penalty` and `mixture` produce better accuracy.





### Exercise 7

Use `select_best()` to choose the model that has the optimal `roc_auc`. Then use `finalize_workflow()`, `fit()`, and `augment()` to fit the model to the training set and evaluate its performance on the testing set.

```{r}
# Use `select_best()` to choose the model that has the optimal `roc_auc`
best_model <- select_best(tune_res, metric = "roc_auc" )
best_model
```

```{r}
# Then use `finalize_workflow()`, `fit()`, and `augment()` to fit the model 
# to the training set and evaluate its performance on the testing set.
elastic_net_final <- finalize_workflow(elastic_net_workflow, best_model)
elastic_net_final_fit <- fit(elastic_net_final, data = pokemon_train)


# first method from the lab
multi_metric <- metric_set(accuracy, sensitivity, specificity)
augment(elastic_net_final_fit, new_data = pokemon_test) %>% multi_metric(truth = type_1, estimate = .pred_class)

# the second method from office hours
augment(elastic_net_final_fit, new_data = pokemon_test) %>%
                  select(type_1,starts_with(".pred"))
```



### Exercise 8

Calculate the overall ROC AUC on the testing set.
```{r}
augment(elastic_net_final_fit, new_data = pokemon_test) %>% roc_auc(type_1, .pred_Bug:.pred_Water)
```

Then create plots of the different ROC curves, one per level of the outcome. Also make a heat map of the confusion matrix.

```{r}
# create plots of the different ROC curves, one per level of the outcome. 
augment(elastic_net_final_fit, new_data = pokemon_test) %>% roc_curve(type_1, .pred_Bug:.pred_Water)%>%autoplot()

# make a heat map of the confusion matrix
augment(elastic_net_final_fit, new_data = pokemon_test) %>% 
  conf_mat(truth = type_1, estimate =.pred_class)%>% 
  autoplot("heatmap")
```


* 1. What do you notice?  
* We notice that the overall ROC AUC on the testing set is 0.73, it is not good enough. Also, from the plots of the different ROC curves, we can see that normal type of Pokemon may have best ROC curves.

* 2. How did your model do? 
* Since the overall ROC AUC on the testing set is 0.73 is less than 0.8, we can conclude that our model didn't do pretty well. This model is not good enough.

* 3. Which Pokemon types is the model best at predicting, and which is it worst at?
*  In order to answer this question, let calculate the number of truth / the number of total 
- for bug: $6/(6+1+3+4+0+2) = 0.375$ 
- for fire: $0/(2+0+0+0+0+0) = 0$ 
- for grass: $1/(1+1+1+1+0+0)= 0.25$
- for normal: $10/(5+1+0+10+3+6)= 0.4$
- for psychic: $6/(0+5+3+1+6+1)= 0.375$
- for water: $13/(2+4+5+5+2+13)= 0.41935$
* From our calculations, we can see that the water type of Pokemon is the model best at predicting, and the fire type of Pokemon is the model worst at predicting.


* 3. Do you have any ideas why this might be?
```{r}
pokemon %>% 
  ggplot(aes(x=type_1))+
  geom_bar()
```


* From this graph, we can know that the fire type of Pokemon has the fewest observations, and the water type of Pokemon has the most observations. It may be the reason why the water type of Pokemon is the model best at predicting, and the fire type of Pokemon is the model worst at predicting.
